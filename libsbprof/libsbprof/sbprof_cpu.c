/* $Id: sbprof_cpu.c,v 1.13 2004/11/23 08:29:06 cgd Exp $ */

/*
 * Copyright 2001, 2003
 * Broadcom Corporation. All rights reserved.
 *
 * This software is furnished under license and may be used and copied only
 * in accordance with the following terms and conditions.  Subject to these
 * conditions, you may download, copy, install, use, modify and distribute
 * modified or unmodified copies of this software in source and/or binary
 * form. No title or ownership is transferred hereby.
 *
 * 1) Any source code used, modified or distributed must reproduce and
 *    retain this copyright notice and list of conditions as they appear in
 *    the source file.
 *
 * 2) No right is granted to use any trade name, trademark, or logo of
 *    Broadcom Corporation.  The "Broadcom Corporation" name may not be
 *    used to endorse or promote products derived from this software
 *    without the prior written permission of Broadcom Corporation.
 *
 * 3) THIS SOFTWARE IS PROVIDED "AS-IS" AND ANY EXPRESS OR IMPLIED
 *    WARRANTIES, INCLUDING BUT NOT LIMITED TO, ANY IMPLIED WARRANTIES OF
 *    MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
 *    NON-INFRINGEMENT ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM BE LIABLE
 *    FOR ANY DAMAGES WHATSOEVER, AND IN PARTICULAR, BROADCOM SHALL NOT BE
 *    LIABLE FOR DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 *    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 *    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 *    BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 *    WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 *    OR OTHERWISE), EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include <exlib.h>

#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include <sb1250-include/sb1250_defs.h>
#include <sb1250-include/sb1250_regs.h>
#include <sb1250-include/sb1250_scd.h>

#include "sbprof_int.h"

static int prof_running;
static int prof_int_was_enabled;


/* must match source of pgather/pkludge */
typedef enum {
  DP_D_PCARCH,
  DP_D_STOPPED,
  DP_D_PID_IMAGE,
  DP_D_PID_LOAD,
  DP_D_PID_DEAD,
  DP_D_CONTEXT,
  DP_D_EVENTS,
  DP_D_CONFIG,
  DP_D_LAST
} dp_driver_msg;
#define EVENT_HDR_MSG_SIZE 4         /* number of uint8_t's per msg */
#define EVENT_MSG_SIZE 4             /* number of uint8_t's per msg */
#define NEW_CONTEXT_MSG_SIZE 12      /* number of uint8_t's per msg */
#define MAX_IMAGE_NAME 256           /* in 8-bit bytes */
#define NEW_PID_MSG_SIZE (32 + MAX_IMAGE_NAME)

/* The 4 definitions below must match those in pgather.c */
#define MAX_COUNTERS 4
#define MAX_SLOTS 8
#define MAX_CPUS 1
#define KERNEL_PID -1

/* Tight loops can have skewed sample distributions when the sampling period
   is fixed.  If the loop and the performance counter interrupt handler
   have no dynamic stalls, one could even see all of the samples landing on a
   single instruction of the loop.

   A solution is to introduce a "ripple" to offset the fixed sampling period
   by a periodic wave.  Ripples are implemented here as a value in the range
   0..RIPPLE_SIZE-1 added to the sampling period.  RIPPLE_SIZE/2 is then
   subtracted to center the wave around the desired sampling period.

   A ripple value is generated by incrementing a previous ripple value by
   RIPPLE_GENERATOR using addition mod RIPPLE_SIZE.  Addition by
   RIPPLE_CONSTANT mod RIPPLE_SIZE must generate all values 0..RIPPLE_SIZE-1.

   If memory were infinite, one could keep the state of a previous ripple for
   each possible instruction PC.  This implementation approximates that by
   direct-mapping the PCs to RIPPLE_TABLE_SIZE distinct distinct ripples.
*/

#define RIPPLE_SIZE (1 << 12)
#define RIPPLE_TABLE_SIZE 1024
#define RIPPLE_GENERATOR (RIPPLE_SIZE/5)
static unsigned short ripple[RIPPLE_TABLE_SIZE];


typedef struct {
  uint64_t total;	/* total number of interrupts */
  uint64_t dropped;	/* total interrupts when buffers were full */
  uint64_t value;	/* initial counter value when slot next entered */
  uint32_t start_period_low;	/* counter val for starting a period */
  uint8_t  start_period_high;
  uint8_t  event;      /* pcarch-specific event_t */
  uint8_t  hwcode;	/* pcarch-specific code for event */
  uint8_t  flags;
#define ECS_INUSE   1   /* Is counter X slot actually used? */
#define ECS_USE_PTR 2   /* Use PTR or restart PC? */

} event_counter_state_t;

typedef struct {
  uint32_t total;	 /* total for current run of slot */
  uint32_t dropped;	 /* dropped for current run of slot */
  uint64_t start_period;/* counter value to start a full period */
} active_counter_state_t;

typedef struct {
  /******* 32-byte boundary *********************************************/
  uint32_t next;/* index into buf for next sample */
  int32_t   last_event; /* index of byte just past last event DP_D_EVENTS
			   message in buf that needs the number of events
			   filled in.  -1 means there is no such message and
			   that such a message must be entered before
			   adding event samples.
			 */
  uint32_t threshold;  /* when does the current multiplexing slot expire? */
  uint8_t slotid;      /* index of current multiplexing slot */
  uint8_t nslots;	/* number of slots */
  uint8_t use_ptr;     /* nonzero IFF counter 1 events get PC from PTR */
  uint8_t pad[17];
  /******* 32-byte boundary *********************************************/
  active_counter_state_t cur_slot[MAX_COUNTERS];
  /******* 32-byte boundary *********************************************/
  event_counter_state_t slot[MAX_SLOTS][MAX_COUNTERS];
  /******* 32-byte boundary *********************************************/
  /* Data buffer goes here, is variable-sized.  */
} cpudata_t;

static cpudata_t *sbprof_cpudata[MAX_CPUS];
static unsigned long sbprof_data_buf_size;

#define cpudata			(sbprof_cpudata[0])
#define cpudata_buf(cdp)	((uint8_t *)(cdp + 1))
#define cpudata_bufsize(cdp)	(sbprof_data_buf_size)

static void slot_enter(cpudata_t *, uint8_t, uint32_t, uint8_t);
static void slot_exit(cpudata_t *, uint8_t, uint64_t *);


/****************************************/
/* Perf Cntr Arch-specific Event Tables */
/****************************************/

#define TRUE (0 == 0)
#define FALSE (!TRUE)

#define BITVECTOR_CONST(x) ((uint8_t) (x))

#define CNTRS_0123 BITVECTOR_CONST(0xf)
#define CNTRS_123  BITVECTOR_CONST(0xe)
#define CNTRS_1    BITVECTOR_CONST(0x2)

typedef struct {
  char     *name;
  uint8_t event;
  uint8_t hwcode;
  uint8_t use_ptr;
  uint8_t period;
  uint8_t counters;
} sbprof_event_entry_t;

static const sbprof_event_entry_t *event_lookup
(const char *name, const sbprof_event_entry_t *tab, int num_events) {
  int i;
  for (i = 0; i < num_events; i++) {
    if (!strcmp(name, tab[i].name)) return &tab[i];
  }
  return NULL;
}

/* Event information for SB-1 CPUs (pass2 and later).  */
static const sbprof_event_entry_t SB1_event_info[] =  {
  /* name   event_t   hwcode  PTR?  period counters */
  { "cycles",   0,      16,   FALSE,  17,  CNTRS_0123 },
  /* Issue Breaks */
  { "ibreak",   1,      39,   TRUE,   19,  CNTRS_1    },
  /* Execution Counts and Slotting */
  { "L0",       2,      40,   TRUE,   19,  CNTRS_1    },
  { "L1",       3,      41,   TRUE,   19,  CNTRS_1    },
  { "E0",       4,      42,   TRUE,   19,  CNTRS_1    },
  { "E1",       5,      43,   TRUE,   19,  CNTRS_1    },
  /* Static and Dynamic Stalls */
  { "issue4",   6,      32,   FALSE,  17,  CNTRS_123  },
  { "noinst",   7,      33,   FALSE,  17,  CNTRS_123  },
  { "dfwait",   8,      34,   FALSE,  19,  CNTRS_123  },
  { "static",   9,      35,   FALSE,  19,  CNTRS_123  },
  { "nopipe",  10,      36,   FALSE,  19,  CNTRS_123  },
  { "ifill",   11,      37,   FALSE,  19,  CNTRS_123  }, /* usually right */
  { "dfill",   12,      38,   FALSE,  19,  CNTRS_123  }, /* broken */
  /* Pipeline Traps */
  { "mispr",   13,      47,   TRUE,   15,  CNTRS_1    },
  { "mboxrp",  14,      29,   FALSE,  15,  CNTRS_123  },
  { "dcfrp",   15,      28,   FALSE,  15,  CNTRS_123  },
  { "deprp",   16,      30,   FALSE,  15,  CNTRS_123  },
  { "fillrp",  17,      27,   FALSE,  15,  CNTRS_123  },
  { "replay",  18,      31,   FALSE,  15,  CNTRS_123  },
  /* Branch Information */
  { "br",      19,      44,   TRUE,   17,  CNTRS_1    },
  { "taken" ,  20,      45,   TRUE,   17,  CNTRS_1    },
  { "ptaken",  21,      46,   TRUE,   17,  CNTRS_1    },
  /* Cache Access */
  { "rqlen",   22,      1,    FALSE,  19,  CNTRS_123  },
  { "urqlen",  23,      2,    FALSE,  19,  CNTRS_123  },
  { "rdmiss",  24,      3,    FALSE,  17,  CNTRS_123  },
  { "dfsne",   25,      10,   FALSE,  17,  CNTRS_123  },
  { "dfs",     26,      11,   FALSE,  17,  CNTRS_123  },
  { "dfn",     27,      12,   FALSE,  17,  CNTRS_123  },
  { "if",      28,      15,   FALSE,  17,  CNTRS_123  },
  { "evict",   29,      13,   FALSE,  17,  CNTRS_123  },
  { "upgrd",   30,      7,    FALSE,  17,  CNTRS_123  },
  { "lshpf",   31,      6,    FALSE,  19,  CNTRS_123  },
  { "pfnop",   32,      5,    FALSE,  19,  CNTRS_123  },
  { "ldhrq",   33,      4,    FALSE,  19,  CNTRS_123  },
  /* BIU */
  { "zbabsy",  34,      17,   FALSE,  19,  CNTRS_123  },
  { "zbdbsy",  35,      18,   FALSE,  19,  CNTRS_123  },
  { "zbareq",  36,      19,   FALSE,  19,  CNTRS_123  },
  { "zbrdn",   37,      20,   FALSE,  19,  CNTRS_123  },
  { "biubsy",  38,      14,   FALSE,  19,  CNTRS_123  },
  /* Multiprocessor */
  { "scfail",  39,      26,   FALSE,  19,  CNTRS_123  },
  { "snphit",  40,      22,   FALSE,  19,  CNTRS_123  },
  { "snpqfl",  41,      23,   FALSE,  19,  CNTRS_123  },
  { "steal",   42,      24,   FALSE,  19,  CNTRS_123  },
  { "steald",  43,      25,   FALSE,  19,  CNTRS_123  },
  /* Instruction Counts */ 
  { "ld",      44,      8,    FALSE,  19,  CNTRS_123  },
  { "st",      45,      9,    FALSE,  19,  CNTRS_123  },
  { "grad",    46,      0,    FALSE,  19,  CNTRS_123  },
  /* Another BIU event */
  { "zbrd",    47,      21,   FALSE,  19,  CNTRS_123  }
};

/******************************************/
/* Perf Cntr Arch-specific Default Events */
/******************************************/

typedef const char *sbprof_default_slot[MAX_COUNTERS];

/* Default events for SB-1 CPUs (pass2 and later).  */
static const sbprof_default_slot SB1_default_slots[] = {
  {"cycles", "ibreak", "dfwait", "static" },
  {"cycles", "mispr",  "issue4", "replay" },
  {"cycles", "L0",     "nopipe", "noinst" },
  {"cycles", "L1",     "rqlen",  "rdmiss" },
  {"cycles", "E0",     "mboxrp", "dcfrp"  },
  {"cycles", "E1",     "deprp",  "fillrp" },
  {"cycles", "br",     "grad",   "rdmiss" }
};

typedef struct {
  const sbprof_default_slot * default_slots;
  int num_slots;
  const sbprof_event_entry_t *event_table;
  const int num_events;
} sbprof_pcarch_info;

static sbprof_pcarch_info pcarch_info[1] = {
  /* SB1 */
  {
    SB1_default_slots,
    sizeof(SB1_default_slots) / sizeof(sbprof_default_slot),
    SB1_event_info,
    sizeof(SB1_event_info) / sizeof (sbprof_event_entry_t)
  }
};

/* Initializes cpudata->slot[s] by looking up the values for event_names in
 * table.  The periods in slots counters will be stored in log base 2.
 * Requires: table has num_events entries
 */
static void parse_mux_slot(const sbprof_default_slot event_names, int s,
			   const sbprof_event_entry_t *table, int num_events)
{
  int i = 0;

  for (i = 0; i < MAX_COUNTERS; i++) {
    event_counter_state_t *ecs = &cpudata->slot[s][i];
    const sbprof_event_entry_t *ev =
            event_lookup(event_names[i], table, num_events);
    if (ev == NULL) {
      printf("sbprof fatal error: %s is not an event name\n", event_names[i]);
      while (1) ;
    }
    /* Check that event e can be counted on counter i */
    if (!(ev->counters & (1 << i))) {
      printf("sbprof fatal error: counter %d cannot count %s\n", i,
	     event_names[i]);
      while (1) ;
    }
    ecs->total = 0;
    ecs->dropped = 0;
    ecs->value = 0;
    ecs->start_period_low = 0;
    ecs->start_period_high = ev->period;
    ecs->event = ev->event;
    ecs->hwcode = ev->hwcode;
    ecs->flags = ECS_INUSE | (ev->use_ptr ? ECS_USE_PTR : 0);
  }
}

/* Converts the periods in slots[] from log base 2 to 64-bit uints.
   The period of event e will be reduced by the factor
     (# of times event e appears in a slot) / nslots */

static
void adjust_slot_periods(int nslots)
{
#define EVENT_MAX 256 /* exceed max event number */
  int occur[EVENT_MAX];
  int e, s, c;
  for (e = 0; e < EVENT_MAX; e++) occur[e] = 0;
  for (s = 0; s < nslots; s++) {
    for (c = 0; c < MAX_COUNTERS; c++) {
      e = cpudata->slot[s][c].event;
      occur[e]++;
    }
  }
  for (s = 0; s < nslots; s++) {
    for (c = 0; c < MAX_COUNTERS; c++) {
      int e = cpudata->slot[s][c].event;
      uint64_t period = 1ULL << cpudata->slot[s][c].start_period_high;

      period *= occur[e];
      period += nslots/2;
      period /= nslots;
#define PERIOD_2_CNTVALUE(p) ((- (int64_t) (p)) & 0x000000ffffffffffULL)
      period = PERIOD_2_CNTVALUE(period);
      /* adjust period by 1/2 ripple so that the mean period will be
	 centered around the current value */
      period -= RIPPLE_SIZE / 2;
      e = cpudata->slot[s][c].event;
      cpudata->slot[s][c].start_period_low = (uint32_t) period;
      cpudata->slot[s][c].start_period_high = (uint8_t) (period >> 32);
    }
  }
}

/* Align byte pointer "p" to next multiple of "n" bytes */
#define ALIGN(p,n) do { long _n = (n); \
                        long _x = (_n-1) & (long) (p); \
                        if (_x) (p) += _n - _x; } \
                   while (0)

/* Initialize the log and config events to monitor. */
static void sbprof_config_log(void) {
  uint32_t cpuid;
  uint8_t pcarch = 4; /* SB1 pass2 and later */
  uint64_t system_rev = *((uint64_t *)(long)0xffffffffb0020000ULL);
  uint8_t *p = cpudata_buf(cpudata);
  sbprof_pcarch_info *pi = &pcarch_info[0];
  int s;

  cpuid = cp0_get_prid ();

  /* Enter pcarch info for pgather */
  *p++ = 5; /* OS_SB1ELF */
  *p++ = pcarch;
  *p++ = 1;
#if defined(MIPSEL)
  *p++ = TRUE;
#elif defined(MIPSEB)
  *p++ = FALSE;
#else
#error "unexpected endianness"
#endif
  *p++ = 0;
  *p++ = 1;
  *p++ = 3; /* major rev */
  *p++ = 0; /* minor rev */
  *(uint64_t *) p = system_rev; p += 8;
  *(uint32_t *) p = cpuid;      p += 4;
  p += 4; /* align to 8 bytes */

  /* set up mux slots */
  if (pi->num_slots > MAX_SLOTS) {
    printf("sbprof: excess slots ignored\n");
    pi->num_slots = MAX_SLOTS;
  }
  for (s = 0; s < pi->num_slots; s++) {
    parse_mux_slot(pi->default_slots[s], s, pi->event_table, pi->num_events);
  }

  /* Enter config info in log */
  *p++ = DP_D_CONFIG;
  *p++ = (uint8_t) pi->num_slots;
  for (s = 0; s < pi->num_slots; s++) {
    int c;
    for (c = 0; c < MAX_COUNTERS; c++) {
      *p++ = cpudata->slot[s][c].event;
      *p++ = (uint8_t) cpudata->slot[s][c].start_period_high;
    }
  }
  ALIGN(p, 8);
  cpudata->nslots = pi->num_slots;
  
  /* Tell pgather to expect -kf for kernel image */
  *p++ = DP_D_PID_IMAGE;
  p += 3;
  *(uint32_t *) p = -1; p += 4;
  *(uint64_t *) p = 1;  p += 8;
  *(uint64_t *) p = 2;  p += 8;
  *(uint32_t *) p = 3;  p += 4;
  *(uint32_t *) p = 0;  p += 4;
  memcpy((void *) p, (void *) "4", 2);
  p += MAX_IMAGE_NAME;
  
  cpudata->next =  p - cpudata_buf(cpudata);
  
  adjust_slot_periods(pi->num_slots);
}



static void unarm_counters(void);

/* READ_COUNTER puts the current count of cpu counter "ctr_num" into "dest".
   WRITE_COUNTER writes the value in "val" to the cpu counter "ctr_num".  */
#ifdef __mips64
#define READ_COUNTER(ctr_num,dest)					\
  __asm__ __volatile__ (".set push ; "					\
			".set mips64 ; "				\
			"dmfc0 %0,$25,%1 ; "				\
			".set pop ; "					\
			: "=r"(dest) : "i"((ctr_num) * 2 + 1))
#define WRITE_COUNTER(ctr_num,val)					\
  __asm__ __volatile__ (".set push ; "					\
			".set mips64 ; "				\
			"dmtc0 %z0,$25,%1 ; "				\
			".set pop ; "					\
			: : "rJ"(val), "i"((ctr_num) * 2 + 1))
#else
#define READ_COUNTER(ctr_num,dest)					\
  ({									\
    uint64_t _v;							\
    long _tmp;								\
    __asm__ __volatile__ (".set push;"					\
			  ".set mips64;"				\
                          ".set noreorder;"				\
			  "dmfc0 %1,$25,%2;"				\
			  "sd    %1,%0;"				\
                          ".set pop;"					\
			  : "=m"(_v), "=r"(_tmp)			\
			  : "i"((ctr_num) * 2 + 1));			\
    (dest) = _v;							\
  })

#define WRITE_COUNTER(ctr_num,val)					\
  ({									\
    uint64_t _v = (val);						\
    long _tmp;								\
    __asm__ __volatile__ (".set push;"					\
			  ".set mips64;"				\
                          ".set noreorder;"				\
			  "ld    %0,%1;"				\
			  "dmtc0 %0,$25,%2;"				\
                          ".set pop"					\
			  : "=r"(_tmp)					\
			  : "m"(_v), "i"((ctr_num) * 2 + 1));		\
  })
#endif

/* Puts "val" into the current control reg of cpu counter "ctr_num" */
#define WRITE_COUNTER_SEL(ctr_num,val)					\
  __asm__ __volatile__ (".set push;"					\
			".set mips64;"					\
			"mtc0 %0,$25,%1;"				\
                        ".set pop"					\
			: : "r"(val), "i"((ctr_num) * 2))

/* disable counter; set value to *value; enable counter */
/* When called with counters frozen, perhaps the disable is unnecessary. */
#define ARM_COUNTER(ctr_num, code, value)				\
  do									\
    {									\
      WRITE_COUNTER_SEL(ctr_num, 1 << 30);				\
      WRITE_COUNTER(ctr_num, value);					\
      WRITE_COUNTER_SEL(ctr_num, code);					\
    }									\
  while (0)

/* Sets counter "counter_num" to monitor event "hwcode" starting from initial
   value "value" */
static __inline__
void arm_counter(uint8_t counter_num, uint32_t hwcode, uint64_t value)
{
  uint32_t code;
  hwcode &= 0x3f;
  code =
    (1 << 30) |		/* freeze enable */
    (hwcode << 5) |	/* event code */
    0x1f;		/* interrupt enable, count always */

  switch(counter_num) {
    case 0: ARM_COUNTER(0, code, value); break;
    case 1: ARM_COUNTER(1, code, value); break;
    case 2: ARM_COUNTER(2, code, value); break;
    case 3: ARM_COUNTER(3, code, value); break;
    default:
	break;
  }
}

#undef ARM_COUNTER

#define UNARM_COUNTER(counter_num) \
  WRITE_COUNTER_SEL(counter_num, 1 << 30); /* freeze enable */ \
  /* Make sure inactive counter doesn't look like it overflowed to prfcntintr \
     handler */ \
  WRITE_COUNTER(counter_num, 0);

static __inline__
void unarm_counters(void)
{
  UNARM_COUNTER(0);
  UNARM_COUNTER(1);
  UNARM_COUNTER(2);
  UNARM_COUNTER(3);
  PAUSE();
}

#undef UNARM_COUNTER

static inline
int report_new_context(cpudata_t *cpud, uint32_t pid, uint8_t cpuid);

static inline void record_sample(uint32_t pc, uint32_t ptr,
				 int counter_num, uint8_t use_ptr)
{
  pc = ((counter_num == 1) && (use_ptr != 0)) ? ptr : pc;

  if (cpudata->next < cpudata_bufsize(cpudata) - 4) {
    *(uint32_t *) &cpudata_buf(cpudata)[cpudata->next] = pc | counter_num;
    cpudata->next += 4;
    if ((cpudata->next % 0x10000) == 0) {
      /* Make sure we don't overflow 16-bit nevents field of EVENTS msg */
      report_new_context(cpudata, KERNEL_PID, 0);
    }
  }
}

/* Called whenever a level 5 interrupt is raised.  Freeze-enable will freeze
   counters IFF a counter has overflowed. */
static void sbprof_cpu_intr(uint64_t restartpc)
{
  int counter_num;
  uint32_t pc = (uint32_t) restartpc;
  uint32_t ptr;
  uint64_t msr[MAX_COUNTERS];
  cpudata_t *cpud = cpudata;
  int cycles_tick;

  __asm__ __volatile__ (".set push;"
			".set mips64;"
			"mfc0 %0, $22,0;"
			".set pop;"
			: "=r"(ptr));
  READ_COUNTER(0, msr[0]);
  READ_COUNTER(1, msr[1]);
  READ_COUNTER(2, msr[2]);
  READ_COUNTER(3, msr[3]);

  ptr = (ptr & 1) ? ptr + 3 : ptr;

  cycles_tick = (msr[0] & (1ULL << 40)) ? 1 : 0;
  for (counter_num = 0; counter_num < 4; counter_num++) {
    if (msr[counter_num] & (1ULL << 40)) {
      uint64_t next_period = cpud->cur_slot[counter_num].start_period;
      int index = (pc >> 3) % RIPPLE_TABLE_SIZE;

      record_sample(pc, ptr, counter_num, cpud->use_ptr);
      next_period += ripple[index];
      /* N.B. start_period was adjusted by 1/2 RIPPLE_SIZE in
	 ioctl SBPROF_START to center ripple around desired period */
      ripple[index] += RIPPLE_GENERATOR;
      ripple[index] &= RIPPLE_SIZE - 1;
      msr[counter_num] = next_period;
    }
  }

  if (cycles_tick) {
    cpud->threshold--;
    if ((cpud->nslots > 1) && !cpud->threshold) {
      cpud->threshold = 16;
      slot_exit(cpud, cpud->slotid, msr);
      slot_enter(cpud,
		 (cpud->slotid + 1) % cpud->nslots,
		 -1,
		 0);
      return;
    }
  }

  /* Case "no counter overflowed":
       The writes below will discard events that occurred during most of
       this routine.
     Case "some counter overflowed":
       For counters that overflowed, these writes unfreeze the counters and
       clear the interrupt request.  For counters that didn't overflow, these
       writes are NOPs since they write back the current value.
       Once the last overflowed counter is written, subsequent writes discard
       some events that might occur as the writes are happening.
  */

  WRITE_COUNTER(0, msr[0]);
  WRITE_COUNTER(1, msr[1]);
  WRITE_COUNTER(2, msr[2]);
  WRITE_COUNTER(3, msr[3]);
}

#define CAUSE_BD  (1 << 31)  /* Mask for BD bit of cause */
static int sbprof_trapfunc(void *arg, struct exframe *ef) {
  uint64_t restart_pc = ef->ef_pc + (ef->ef_cause & CAUSE_BD ? 4 : 0);
  
  sbprof_cpu_intr(restart_pc);

  /* Knock down count/compare */
  cp0_set_compare (cp0_get_compare ());

  // indicate that the interrupt was handled
  return 1;
}

/* If cpud's curbuf has an EVENT_HDR message lacking nevents, fills in nevents.
   (Unless nevents would be 0, in which case the EVENT_HDR message is
   removed from the buffer.)
   Sets last_event to -1.
*/
static void backfill_nevents(cpudata_t *cpud)
{
  if (cpud->last_event >= 0) {
    if (cpud->next == cpud->last_event) {
      cpud->next -= EVENT_HDR_MSG_SIZE;
    } else {
      int idx = cpud->last_event - 2;
      uint16_t n = (cpud->next - cpud->last_event) / EVENT_MSG_SIZE;
      uint16_t *p;
      p = (uint16_t *) &cpudata_buf(cpud)[idx];
      *p = n;
    }
    cpud->last_event = -1;
  }
}

/***************************************************
 * Interrupt Invariant (holds whenver interrupts enabled)
 *  XOR
 *   o last_event == -1
 *   o cpudata.buf has a previous EVENT_HDR message
 *
 ***************************************************/

/* Requires: cpud->buf has room for event header and event
             p points to next buf entry
   Effect:   adds event header */
static inline
void enter_event_hdr(cpudata_t *cpud, uint8_t *p)
{
  *p++ = DP_D_EVENTS;
  cpud->next += EVENT_HDR_MSG_SIZE;
  cpud->last_event = cpud->next;
}

/* Requires: cpud->last_event == -1
   Effect:   Enters a new_context message followed by an event_hdr message */
static inline
void enter_context(cpudata_t *cpud, uint32_t pid, uint8_t cpuid)
{
  uint8_t *p = (uint8_t *) &cpudata_buf(cpud)[cpud->next];

  *p++ = DP_D_CONTEXT;
  *p++ = cpuid;
  *p++ = cpud->slotid;
  p += 1; /* align to 4 bytes */
  *((uint32_t *) p) = pid; p += 4;
  *((uint32_t *) p) = 0; p += 4;
  cpud->next += NEW_CONTEXT_MSG_SIZE;
  enter_event_hdr(cpud, p);
}  

/* Effect: If there's enough room in a single, non-full cpud buffer for a
           new_context message followed by a pc sample, enters the new_context
	   message + event_hdr message and returns 0; else returns 1.
 */
static inline
int report_new_context(cpudata_t *cpud, uint32_t pid, uint8_t cpuid)
{
  backfill_nevents(cpud);
  if (cpud->next > cpudata_bufsize(cpud) - (NEW_CONTEXT_MSG_SIZE +
					    EVENT_HDR_MSG_SIZE +
					    EVENT_MSG_SIZE)) {
    return 1;
  }
  enter_context(cpud, pid, cpuid);
  return 0;
}

static __inline__ uint64_t u64max(uint64_t a, uint64_t b) {
  return (a > b) ? a : b;
}

static void slot_enter(cpudata_t *cpup, uint8_t slotid, uint32_t pid,
		       uint8_t cpuid)
{
  int i;
  event_counter_state_t *ecs = cpup->slot[slotid];

  /* XXX add prefetching, avoid branch? */
  for (i = 0; i < MAX_COUNTERS; i++) {
    /* Zero the per-slot-activation counts */
    cpup->cur_slot[i].total = 0;
    cpup->cur_slot[i].dropped = 0;
    if (ecs[i].flags & ECS_INUSE) {
      /* Compute value for re-arming counter */
      cpup->cur_slot[i].start_period =
	((uint64_t) ecs[i].start_period_low) |
	(((uint64_t) ecs[i].start_period_high) << 32);
      /* Set up counters with values from exit of last activation */
      arm_counter(i, ecs[i].hwcode, u64max(ecs[i].value,
					   cpup->cur_slot[i].start_period));
    }
  }
  cpup->slotid = slotid;
  cpup->use_ptr = ecs[1].flags & ECS_USE_PTR;
  report_new_context(cpup, pid, cpuid);
}

static void slot_exit(cpudata_t *cpup, uint8_t slotid, uint64_t *counter_val)
{
  int i;
  event_counter_state_t *ecs = cpup->slot[slotid];

  /* XXX add prefetching, avoid branch? */
  for (i = 0; i < MAX_COUNTERS; i++) {
    if (ecs[i].flags & ECS_INUSE) {
      /* Export the per-slot-activation counts */
      ecs[i].total += cpup->cur_slot[i].total;
      ecs[i].dropped += cpup->cur_slot[i].dropped;
      /* Remember where to start off next time */
      ecs[i].value = counter_val[i];
    }
  }
  unarm_counters();
}

struct ihand sbprof_intr_handle;

int
sbprof_cpu_init (long bufsize)
{
  char *buf;

  if (bufsize == SBPROF_BUFSIZE_DEFAULT)
    bufsize = SBPROF_CPU_BUFSIZE_DEFAULT;

  sbprof_data_buf_size = bufsize; 

  bufsize += sizeof (cpudata_t);

  /* Allocate the buffer plus a little bit, so we can make sure it's cache
     line aligned.  We use calloc so that the buffer is filled with zero,
     which is important because the "cpudata_t" in the buffer keeps the
     state about whether data has been recorded.  If it starts out with
     random data, we might report data as haveing been recorded when
     really none was.  */
  buf = calloc (1, bufsize + 31);
  if (buf == NULL)
    return -1;

  /* Align to a cache line boundary.  */
  cpudata = (void *)(((long)buf + 31) & ~31);

  return 0;
}

void
sbprof_get_cpu_buf (void **bufpp, size_t *sizep)
{

  if (cpudata == NULL)
    {
      *bufpp = NULL;
      *sizep = 0;
    }
  else
    {
      *bufpp = cpudata_buf (cpudata);
      *sizep = cpudata->next;
    }
}

void sbprof_start(void) {
  int i;

  if (!cpudata)
    return;

  disable_intr();

  if (prof_running)
    goto out;
  prof_running = 1;

  /* Initialize ripple start points to random values */
  for (i = 0; i < RIPPLE_TABLE_SIZE; i++) {
    static uint64_t rand = 0;
    ripple[i] = rand & (RIPPLE_SIZE - 1);
    rand = (rand * 0x5deece66dULL + 0xb) & 0xffffffffffffULL;
  }
  
  sbprof_config_log();
  cpudata->threshold = 16;
  cpudata->slotid = 0;
  cpudata->last_event = -1;

  unarm_counters();
  slot_enter(cpudata, 0, KERNEL_PID, 0);
  
  INIT_IHAND(&sbprof_intr_handle, sbprof_trapfunc, NULL, EXFPSAVEMASK_NONE);
  intr_handler_add(7, &sbprof_intr_handle);

  /* Unmask I5/IM7 interrupts */
  prof_int_was_enabled = intr_enable_int(NULL, 7);

out:
  enable_intr();
}

void sbprof_stop() {
  uint64_t counter_val[MAX_COUNTERS];

  disable_intr();

  if (!prof_running)
    goto out;
  prof_running = 0;

  unarm_counters();
  enable_intr();
  /* Wait for any last interrupts */
  PAUSE();
  PAUSE();

  disable_intr ();

  if (!prof_int_was_enabled)
    intr_disable_int(NULL, 7);
  intr_handler_remove(7, &sbprof_intr_handle);

  READ_COUNTER(0, counter_val[0]);
  READ_COUNTER(1, counter_val[1]);
  READ_COUNTER(2, counter_val[2]);
  READ_COUNTER(3, counter_val[3]);
  slot_exit(cpudata, cpudata->slotid, counter_val);
  backfill_nevents(cpudata);

out:
  enable_intr();
}
